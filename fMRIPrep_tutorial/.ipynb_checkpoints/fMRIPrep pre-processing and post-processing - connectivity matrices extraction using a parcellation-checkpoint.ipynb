{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRIPrep pre-processing and post-processing - connectivity matrices extraction using a parcellation\n",
    "\n",
    "This notebook is intented to give a brief overview of a pipeline to:\n",
    "\n",
    "1) Download data from Beluga, an HPC  \n",
    "2) Verify the data with bids-validator using a Docker container  \n",
    "3) Run the pre-processing with fMRIPrep using a Docker container  \n",
    "4) Create and run a post-processing pipeline in Nilearn to obtain connectivity matrices\n",
    "\n",
    "I tend to be quite verbose in the descriptions below. Please skip to the section's tl;dr to simply get the code and a brief description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The data used in this tutorial is anonymized data from the Prevent-AD Cohort. Data from the Prevent-AD is open and available at this [address](https://portal.conp.ca/dataset?id=projects/preventad-open). Unfortunately, the images from the cohort on the CONP are only available in .mnc format. While .mnc images are tranformable to .nii format, they are not transformable in BIDS at this time.\n",
    "\n",
    "As such, the tutorial used closed data from the Prevent-AD cohort which was readily available in BIDS format. As the data is not open, the participant ID was anonymized and the data is not available to be reproduced.\n",
    "\n",
    "However, the tools used and the tutorial should be applicable to any bids-validated datasets, which is why we run the bids-validator first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Downloading data from Beluga\n",
    "\n",
    "In this step, we simply download the data from Beluga. The exact code is not shared as it would leak the actual participant ID. However, I share below the command that would be used to download the data. \n",
    "\n",
    "```scp -r user@beluga.computecanada.ca:/path/to/directory/to/copy /directory/local/computer/to/copy```\n",
    "\n",
    "In this ```scp``` command, the ```-r``` argument serves to copy whole directories to a local computer. It is important to put ```:``` after your user name for the HPC. The path after the ```:``` refers to the path on the supercomputer that needs to be copied to the local computer. After this path, a white space and a path refering to the folder on the local computer where the folder from the HPC should be copied.\n",
    "\n",
    "Should you want to copy a folder from your computer to the HPC (e.g. after the pre-processing), you simply need to invert the order of the paths.\n",
    "\n",
    "```scp -r /directory/local/computer/to/copy/to/HPC user@beluga.computecanada.ca:/path/to/remote/directory/```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Verify BIDS validity with bids-validator\n",
    "\n",
    "Once the data is downloaded, and in our case, anonymized, we are ready to validate our bids! You can find everything there is to know about BIDS [here](https://bids-specification.readthedocs.io/en/stable/). Note that the actual bids-specification version might differ slightly from the version available within the Docker container.\n",
    "\n",
    "While this might seem redundant when running the bids-validator on our local computer (since fMRIPrep also runs the bids-validator before launching the pre-processing), it becomes particularly useful when we need to launch jobs say on remote HPCs. We might not receive a notice right away that our pre-processing failed because our data is not in bids format for example. Further it is a good practice on how to use containers. You can note however that there is a bunch of different ways to download and use bids-validator as described [here](https://pypi.org/project/bids-validator/), including a [web browser](http://bids-standard.github.io/bids-validator/) where you can upload the dataset and verify it there.\n",
    "\n",
    "For the Docker container, you first need to install [Docker](https://hub.docker.com/editions/community/docker-ce-desktop-mac/). The hyperlink shows instructions for Mac, but Docker is available for Windows and Linux. Once you have gone through the instructions, your Docker version should be ready to go! \n",
    "\n",
    "To use the bids-validator, we first need to ```pull``` the Docker image of the bids-validator. In short, by pulling, we are \"installing\" the software on our computer, without actually installing software on the hardware. This might not make a ton of sense, but for now, you can think of it as accessing another computer, that is not yours, which only contains the softwares necessary to run bids-validator.\n",
    "\n",
    "The first time you run the command below, Docker will ```pull``` the Docker image for you and run the analyses right away. Note that I split the code below in 4 lines for readability using back slashes, but you can run this command in a single line too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker run -ti --rm \\\n",
    "#-v /path/to/data:/data:ro \\\n",
    "#bids/validator \\\n",
    "#/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack the command. \n",
    "\n",
    "The first line: Calls Docker and tells it to run in interactive mode (```ti```) (i.e. once we run Docker, we will be \"warped\" inside the container where there will be an output displayed on the terminal as the software runs. We also use the ```--rm``` command to \"clean\" our environment before the container is called. This insures that no variables from our Unix/Mac environment \"leak\" in the container. It is basically just how you would clean a wet table before putting a cardboard on it so that the water wouldn't leak in the box.\n",
    "\n",
    "The second line: This is called a \"mount\" and is called in Docker using the ```-v``` argument. Is it telling Docker where on our computer it can fetch the data we want the software inside the container to analyze. When using the ```-v``` argument, we need to tell it: 1) Where to find the data on our computer, 2) How to call this path in the Container and 3) Whether or not Docker has permissions to modify the files in this folder (in this case, ```ro``` stands for read-only. To summarize, this mount tells Docker that the data is on our computer at a certain path (i.e. ```path/to/data```). Then, it tells Docker that inside the container, we should refer to it as ```/data```. Finally, we tell Docker that it can't modify this data: it is read only.\n",
    "\n",
    "The third line: This is straightforward-this is simply the program Docker needs to call from within the container.\n",
    "\n",
    "The fourth line: This is an argument given to the program 'bids-validator'. In this case, the program looks for a BIDS dataset within the path you gave it.\n",
    "\n",
    "Once run, the command will open in your terminal in 'interactive' mode. You will see "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker run -ti --rm \\\n",
    "#-v /Users/stong3/Desktop/test_fmriprep_PAD/sourcedata:/data:ro \\\n",
    "#bids/validator \\\n",
    "#/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this command gives us an output that looks a little bit like this:\n",
    "\n",
    "IMAGE COMMAND LINE\n",
    "\n",
    "In red, we have errors: These are things that will be problematic when trying to run the bids apps (in our case, fMRIPrep). Full disclosure, in Prevent-AD, it seems that the field 'TaskName' is missing from our .json files. As such, bids-validator will throw an error. However, fMRIPrep still ran in our case.\n",
    "\n",
    "In blue: we have references that the bids-validator recommend to check to get more information on the error. Note that these links do not always work, as they are auto-generated, so a Google search is much better.\n",
    "\n",
    "In greenish/yellow: we have warning. These warnings are 'recommendations' that the bids specification asks for. However, they are not essential for the code to run properly. \n",
    "\n",
    "Once we verified our BIDS dataset and corrected the errors, we can re-run it again to insure that it is completly bids-compliant. Then, we are ready for our pre-processing!\n",
    "\n",
    "### tl;dr - bids-validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker run -ti --rm \\\n",
    "#-v /Users/stong3/Desktop/test_fmriprep_PAD/sourcedata:/data:ro \\\n",
    "#bids/validator \\\n",
    "#/data\n",
    "\n",
    "#########\n",
    "# Mount option to change. In your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Run fMRIPrep pre-processing\n",
    "\n",
    "As we now know how to use Docker containers, this next part should not be too difficult. To run fMRIPrep, a single command line is necessary. However, I would recommend to edit this command in a text editor first so that you can make sure that all the arguments necessary are there. The goal here is not to describe what fMRIPrep **does** in terms of pre-processing. The [fMRIPrep documentation](https://fmriprep.readthedocs.io/en/stable/), though quite long, is quite thorough in its documentation. The goal is to describe the correct command(s) to go through the pre-processing. \n",
    "\n",
    "fMRIPrep gives a few options to download and use the software, but their recommended method is to use a Docker container and use their Python script wrapper to simplify the command complexity (i.e. you do not need to call Docker as the script will do it for you). However, running the Docker container directly will give you a lot more option to fine-tune your analyses. Here is an example of a command from the [fMRIPrep documentation](https://fmriprep.readthedocs.io/en/stable/docker.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker run -ti --rm \\\n",
    "#-v path/to/data:/data:ro \\\n",
    "#-v path/to/output:/out \\\n",
    "#poldracklab/fmriprep:<latest-version> \\\n",
    "#/data /out/out \\\n",
    "#participant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack the command.\n",
    "\n",
    "<ins>**The first line:**</ins> Simply calls Docker in interactive mode and cleans the environment, as we have seen with the bids-validator.\n",
    "\n",
    "<ins>**The second line:**</ins> A mount telling fMRIPrep where to get the data on our computer and telling it that it can't modify these files in their original folders (with the ```:ro```) option. We will call it ```/data``` in the container.\n",
    "\n",
    "<ins>**The third line:**</ins> A mount telling fMRIPrep where to send back the pre-processed data on our computer. We will call it ```/out/``` in the container.\n",
    "\n",
    "<ins>**The fourth line:**</ins> Calls fMRIPrep. The next lines will be arguments that we give directly to fMRIPrep to specify what and how we want to process our data.\n",
    "\n",
    "<ins>**The fifth line:**</ins> We tell fMRIPrep 2 things. 1) The data to analyze is in the ```/data``` folder, which we defined with a mount before and the output where the data is to be store is the ```/out``` folder which we also defined in a mount. The second ```out``` is basically to create a separate folder for fMRIPrep in the output folder.\n",
    "\n",
    "<ins>**The sixth line:**</ins> We tell fMRIPrep which participants we want to pre-process.\n",
    "\n",
    "------------\n",
    "\n",
    "These are the basic arguments that fMRIPrep needs to pre-process the data. The full list of arguments that can be used can be found [here](https://fmriprep.readthedocs.io/en/stable/usage.html).\n",
    "\n",
    "In the case of the pre-processing I did for the current tutorial, I ran the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker run -it --rm \\\n",
    "#-v /Users/stong3/Desktop/test_fmriprep_PAD/sourcedata:/data:ro \\\n",
    "#-v /Users/stong3/Desktop/test_fmriprep_PAD/derivative:/out \\\n",
    "#-v /Users/stong3/Desktop/test_fmriprep_PAD/fs_license/license.txt:/opt/freesurfer/license.txt \\\n",
    "#-v /Users/stong3/Desktop/test_fmriprep_PAD/work_dir:/work \\\n",
    "#poldracklab/fmriprep:latest \\\n",
    "#/data /out/fmriprep \\\n",
    "#participant \\\n",
    "#--participant-label sub-00001 \\\n",
    "#-w /work \\\n",
    "#--low-mem \\\n",
    "#--output-spaces T1w \\\n",
    "#--write-graph \\\n",
    "#--fs-license-file /opt/freesurfer/license.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what changed?\n",
    "\n",
    "<ins>**The first, second and third lines:**</ins> This is unchanged from the basic structure, i.e., we run Docker, and give an input and output mount. \n",
    "\n",
    "<ins>**The fourth line:**</ins> This line tells Docker where to find a Freesurfer license. As part of the fMRIPrep processing, FreeSurfer is run on all anatomical images available to render surfaces and in part for registration of the anatomical template to the functional template. To do this, fMRIPrep needs to access a license that authorizes users to use FreeSurfer. This is free and can be done [here](https://surfer.nmr.mgh.harvard.edu/registration.html). \n",
    "\n",
    "<ins>**The fifth line:**</ins> This mount is to make it a bit easier on your computer to process the data. It creates a work directory where the intermediate output of the pre-processing are stored during processing so that fMRIPrep doesn't store all of it the computer's memory. We will define the work directory a little bit later.\n",
    "\n",
    "<ins>**The sixth and seventh lines:**</ins> These are the basic fMRIPrep arguments where we start the software, where we tell it where to find the data, where to output the pre-processed files.\n",
    "\n",
    "<ins>**The eight and nine lines:**</ins> This is where we define the participants we want to process. We first tell it the ```participant``` argument, followed by the ```--participant-label```. We then feed it the labels of the participants we want to process. In our case, we simply want the subject 00001. \n",
    "\n",
    "<ins>**The tenth line:**</ins> This defines the work directory where to put the intermediate files. See the comment above the **fifth line** above. \n",
    "\n",
    "<ins>**The eleventh line:**</ins> This tells fMRIPrep that our computer does not have a lot of RAM, and to go a bit easier on it. This is particularly relevant when running fMRIPrep on a local computer.\n",
    "\n",
    "<ins>**The twelveth line:**</ins> We now chose in what space do we want our processing to be outputed. This depends on the type of analyses we are interested in. Normally, the final images are spatially registered to the MNI atlas. In our case, since the analyses were to done with the mentality of using the output for fMRI fingerprinted, I chose to register in T1w space (i.e. in the space of their T1w scans). You can ask fMRIPrep to give\n",
    "\n",
    "<ins>**The final line:**</ins> This tells fMRIPrep where to find the FreeSurfer license, as described above. It is worth noting that existing FreeSurfer outputs (from version 6.0.0 onwards) can be fed to fMRIPrep without running the whole recon-all pipeline. Since we mounted a directory with the Freesurfer license, we need to tell it where is the license. **Be careful:** Here, the path does not refer to the path on your computer, **but to the path inside the container that you mounted earlier in line 4.**\n",
    "\n",
    "Do not forget:\n",
    "\n",
    "1) Spaces and indents are very sensitive in bash, so make sure everything is written exactly  \n",
    "2) Make sure that there is no typo. I would advice copy/pasting directly from your terminal the paths using ```pwd``` to make sure that you are in the correct place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "Should everything go right, your terminal should soon change and start processing the data. A lot of text should scroll regularly in your terminal. It is worth noting that in a couple of cases, depending on your computer, certain processing steps might appear \"jammed\" in the same command for a couple of hours (particularly ```resume recon-all```, the Freesurfer command. For information, here is the specs of the computer used to run the code above:\n",
    "\n",
    "macOS Mojave\n",
    "Version 10.14.6\n",
    "iMac (Retina 5K, 27-inch, Late 2015)\n",
    "3.2 GHz Intel Core i5\n",
    "8 GB 1867 MHz DDR3\n",
    "AMD Radeon R9 M380 2 GB\n",
    "\n",
    "Running fMRIPrep on this computer, for a single subject, 2 visits, took 14h, allocating 5.67GB of RAM to the processing. \n",
    "\n",
    "**Of note:** Docker by default allows only 4GB of RAM to be allocated to programs running in containers. This can be changed in Docker > Preferences > Ressources > Memory. I allowed Docker to use up to 6GB on the computer.\n",
    "\n",
    "---------\n",
    "\n",
    "Ok! Now, we have run fMRIPrep and we have a message in the terminal indicating that fMRIPrep proceeded successfully. Great! As a reminder, here are the files we started with before the run:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sourcedata/\n",
    "├── dataset_description.json\n",
    "└── sub-00001\n",
    "    ├── ses-BL00A\n",
    "    │   ├── anat\n",
    "    │   │   ├── sub-00001_ses-BL00A_T1w.json\n",
    "    │   │   └── sub-00001_ses-BL00A_T1w.nii.gz\n",
    "    │   └── func\n",
    "    │       ├── sub-00001_ses-BL00A_task-rest_run-1_bold.json\n",
    "    │       └── sub-00001_ses-BL00A_task-rest_run-1_bold.nii.gz\n",
    "    └── ses-FU12A\n",
    "        ├── anat\n",
    "        │   ├── sub-00001_ses-FU12A_T1w.json\n",
    "        │   └── sub-00001_ses-FU12A_T1w.nii.gz\n",
    "        └── func\n",
    "            ├── sub-00001_ses-FU12A_task-rest_run-1_bold.json\n",
    "            └── sub-00001_ses-FU12A_task-rest_run-1_bold.nii.gz\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, here's our output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "derivatives/\n",
    "└── fmriprep\n",
    "    ├── fmriprep\n",
    "    │   ├── dataset_description.json\n",
    "    │   ├── desc-aparcaseg_dseg.tsv\n",
    "    │   ├── desc-aseg_dseg.tsv\n",
    "    │   ├── logs\n",
    "    │   │   ├── CITATION.bib\n",
    "    │   │   ├── CITATION.html\n",
    "    │   │   ├── CITATION.md\n",
    "    │   │   └── CITATION.tex\n",
    "    │   ├── sub-00001\n",
    "    │   │   ├── anat\n",
    "    │   │   │   ├── sub-00001_desc-aparcaseg_dseg.nii.gz\n",
    "    │   │   │   ├── sub-00001_desc-aseg_dseg.nii.gz\n",
    "    │   │   │   ├── sub-00001_desc-brain_mask.json\n",
    "    │   │   │   ├── sub-00001_desc-brain_mask.nii.gz\n",
    "    │   │   │   ├── sub-00001_desc-preproc_T1w.json\n",
    "    │   │   │   ├── sub-00001_desc-preproc_T1w.nii.gz\n",
    "    │   │   │   ├── sub-00001_dseg.nii.gz\n",
    "    │   │   │   ├── sub-00001_from-MNI152NLin2009cAsym_to-T1w_mode-image_xfm.h5\n",
    "    │   │   │   ├── sub-00001_from-T1w_to-MNI152NLin2009cAsym_mode-image_xfm.h5\n",
    "    │   │   │   ├── sub-00001_from-T1w_to-fsnative_mode-image_xfm.txt\n",
    "    │   │   │   ├── sub-00001_from-fsnative_to-T1w_mode-image_xfm.txt\n",
    "    │   │   │   ├── sub-00001_hemi-L_inflated.surf.gii\n",
    "    │   │   │   ├── sub-00001_hemi-L_midthickness.surf.gii\n",
    "    │   │   │   ├── sub-00001_hemi-L_pial.surf.gii\n",
    "    │   │   │   ├── sub-00001_hemi-L_smoothwm.surf.gii\n",
    "    │   │   │   ├── sub-00001_hemi-R_inflated.surf.gii\n",
    "    │   │   │   ├── sub-00001_hemi-R_midthickness.surf.gii\n",
    "    │   │   │   ├── sub-00001_hemi-R_pial.surf.gii\n",
    "    │   │   │   ├── sub-00001_hemi-R_smoothwm.surf.gii\n",
    "    │   │   │   ├── sub-00001_label-CSF_probseg.nii.gz\n",
    "    │   │   │   ├── sub-00001_label-GM_probseg.nii.gz\n",
    "    │   │   │   ├── sub-00001_label-WM_probseg.nii.gz\n",
    "    │   │   │   ├── sub-00001_space-MNI152NLin2009cAsym_desc-brain_mask.json\n",
    "    │   │   │   ├── sub-00001_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz\n",
    "    │   │   │   ├── sub-00001_space-MNI152NLin2009cAsym_desc-preproc_T1w.json\n",
    "    │   │   │   ├── sub-00001_space-MNI152NLin2009cAsym_desc-preproc_T1w.nii.gz\n",
    "    │   │   │   ├── sub-00001_space-MNI152NLin2009cAsym_dseg.nii.gz\n",
    "    │   │   │   ├── sub-00001_space-MNI152NLin2009cAsym_label-CSF_probseg.nii.gz\n",
    "    │   │   │   ├── sub-00001_space-MNI152NLin2009cAsym_label-GM_probseg.nii.gz\n",
    "    │   │   │   └── sub-00001_space-MNI152NLin2009cAsym_label-WM_probseg.nii.gz\n",
    "    │   │   ├── figures\n",
    "    │   │   │   ├── sub-00001_desc-conform_T1w.html\n",
    "    │   │   │   ├── sub-00001_desc-reconall_T1w.svg\n",
    "    │   │   │   ├── sub-00001_dseg.svg\n",
    "    │   │   │   ├── sub-00001_ses-BL00A_desc-about_T1w.html\n",
    "    │   │   │   ├── sub-00001_ses-BL00A_desc-summary_T1w.html\n",
    "    │   │   │   ├── sub-00001_ses-BL00A_task-rest_run-1_desc-bbregister_bold.svg\n",
    "    │   │   │   ├── sub-00001_ses-BL00A_task-rest_run-1_desc-carpetplot_bold.svg\n",
    "    │   │   │   ├── sub-00001_ses-BL00A_task-rest_run-1_desc-compcorvar_bold.svg\n",
    "    │   │   │   ├── sub-00001_ses-BL00A_task-rest_run-1_desc-confoundcorr_bold.svg\n",
    "    │   │   │   ├── sub-00001_ses-BL00A_task-rest_run-1_desc-rois_bold.svg\n",
    "    │   │   │   ├── sub-00001_ses-BL00A_task-rest_run-1_desc-summary_bold.html\n",
    "    │   │   │   ├── sub-00001_ses-BL00A_task-rest_run-1_desc-validation_bold.html\n",
    "    │   │   │   ├── sub-00001_ses-FU12A_task-rest_run-1_desc-bbregister_bold.svg\n",
    "    │   │   │   ├── sub-00001_ses-FU12A_task-rest_run-1_desc-carpetplot_bold.svg\n",
    "    │   │   │   ├── sub-00001_ses-FU12A_task-rest_run-1_desc-compcorvar_bold.svg\n",
    "    │   │   │   ├── sub-00001_ses-FU12A_task-rest_run-1_desc-confoundcorr_bold.svg\n",
    "    │   │   │   ├── sub-00001_ses-FU12A_task-rest_run-1_desc-rois_bold.svg\n",
    "    │   │   │   ├── sub-00001_ses-FU12A_task-rest_run-1_desc-summary_bold.html\n",
    "    │   │   │   ├── sub-00001_ses-FU12A_task-rest_run-1_desc-validation_bold.html\n",
    "    │   │   │   └── sub-00001_space-MNI152NLin2009cAsym_T1w.svg\n",
    "    │   │   ├── log\n",
    "    │   │   │   └── 20200603-214135_2ed1f184-65b2-4aec-86a8-a8f1877190df\n",
    "    │   │   │       └── fmriprep.toml\n",
    "    │   │   ├── ses-BL00A\n",
    "    │   │   │   ├── anat\n",
    "    │   │   │   │   └── sub-00001_ses-BL00A_from-orig_to-T1w_mode-image_xfm.txt\n",
    "    │   │   │   └── func\n",
    "    │   │   │       ├── sub-00001_ses-BL00A_task-rest_run-1_desc-confounds_regressors.json\n",
    "    │   │   │       ├── sub-00001_ses-BL00A_task-rest_run-1_desc-confounds_regressors.tsv\n",
    "    │   │   │       ├── sub-00001_ses-BL00A_task-rest_run-1_space-T1w_boldref.nii.gz\n",
    "    │   │   │       ├── sub-00001_ses-BL00A_task-rest_run-1_space-T1w_desc-aparcaseg_dseg.nii.gz\n",
    "    │   │   │       ├── sub-00001_ses-BL00A_task-rest_run-1_space-T1w_desc-aseg_dseg.nii.gz\n",
    "    │   │   │       ├── sub-00001_ses-BL00A_task-rest_run-1_space-T1w_desc-brain_mask.json\n",
    "    │   │   │       ├── sub-00001_ses-BL00A_task-rest_run-1_space-T1w_desc-brain_mask.nii.gz\n",
    "    │   │   │       ├── sub-00001_ses-BL00A_task-rest_run-1_space-T1w_desc-preproc_bold.json\n",
    "    │   │   │       └── sub-00001_ses-BL00A_task-rest_run-1_space-T1w_desc-preproc_bold.nii.gz\n",
    "    │   │   └── ses-FU12A\n",
    "    │   │       ├── anat\n",
    "    │   │       │   └── sub-00001_ses-FU12A_from-orig_to-T1w_mode-image_xfm.txt\n",
    "    │   │       └── func\n",
    "    │   │           ├── sub-00001_ses-FU12A_task-rest_run-1_desc-confounds_regressors.json\n",
    "    │   │           ├── sub-00001_ses-FU12A_task-rest_run-1_desc-confounds_regressors.tsv\n",
    "    │   │           ├── sub-00001_ses-FU12A_task-rest_run-1_space-T1w_boldref.nii.gz\n",
    "    │   │           ├── sub-00001_ses-FU12A_task-rest_run-1_space-T1w_desc-aparcaseg_dseg.nii.gz\n",
    "    │   │           ├── sub-00001_ses-FU12A_task-rest_run-1_space-T1w_desc-aseg_dseg.nii.gz\n",
    "    │   │           ├── sub-00001_ses-FU12A_task-rest_run-1_space-T1w_desc-brain_mask.json\n",
    "    │   │           ├── sub-00001_ses-FU12A_task-rest_run-1_space-T1w_desc-brain_mask.nii.gz\n",
    "    │   │           ├── sub-00001_ses-FU12A_task-rest_run-1_space-T1w_desc-preproc_bold.json\n",
    "    │   │           └── sub-00001_ses-FU12A_task-rest_run-1_space-T1w_desc-preproc_bold.nii.gz\n",
    "    │   └── sub-00001.html\n",
    "    └── freesurfer\n",
    "        ├── fsaverage\n",
    "        │   ├── label\n",
    "        │   │   ├── Yeo_Brainmap_fsaverage_README\n",
    "        │   │   ├── lh.BA1_exvivo.label\n",
    "        │   │   ├── lh.BA1_exvivo.thresh.label\n",
    "        │   │   ├── lh.BA2_exvivo.label\n",
    "        │   │   ├── lh.BA2_exvivo.thresh.label\n",
    "        │   │   ├── lh.BA3a_exvivo.label\n",
    "        │   │   ├── lh.BA3a_exvivo.thresh.label\n",
    "        │   │   ├── lh.BA3b_exvivo.label\n",
    "        │   │   ├── lh.BA3b_exvivo.thresh.label\n",
    "        │   │   ├── lh.BA44_exvivo.label\n",
    "        │   │   ├── lh.BA44_exvivo.thresh.label\n",
    "        │   │   ├── lh.BA45_exvivo.label\n",
    "        │   │   ├── lh.BA45_exvivo.thresh.label\n",
    "        │   │   ├── lh.BA4a_exvivo.label\n",
    "        │   │   ├── lh.BA4a_exvivo.thresh.label\n",
    "        │   │   ├── lh.BA4p_exvivo.label\n",
    "        │   │   ├── lh.BA4p_exvivo.thresh.label\n",
    "        │   │   ├── lh.BA6_exvivo.label\n",
    "        │   │   ├── lh.BA6_exvivo.thresh.label\n",
    "        │   │   ├── lh.MT_exvivo.label\n",
    "        │   │   ├── lh.MT_exvivo.thresh.label\n",
    "        │   │   ├── lh.Medial_wall.label\n",
    "        │   │   ├── lh.PALS_B12.labels.gii\n",
    "        │   │   ├── lh.PALS_B12_Brodmann.annot\n",
    "        │   │   ├── lh.PALS_B12_Lobes.annot\n",
    "        │   │   ├── lh.PALS_B12_OrbitoFrontal.annot\n",
    "        │   │   ├── lh.PALS_B12_Visuotopic.annot\n",
    "        │   │   ├── lh.V1_exvivo.label\n",
    "        │   │   ├── lh.V1_exvivo.thresh.label\n",
    "        │   │   ├── lh.V2_exvivo.label\n",
    "        │   │   ├── lh.V2_exvivo.thresh.label\n",
    "        │   │   ├── lh.Yeo2011_17NetworksConfidence_N1000.mgz\n",
    "        │   │   ├── lh.Yeo2011_17Networks_N1000.annot\n",
    "        │   │   ├── lh.Yeo2011_7NetworksConfidence_N1000.mgz\n",
    "        │   │   ├── lh.Yeo2011_7Networks_N1000.annot\n",
    "        │   │   ├── lh.Yeo_Brainmap_10Comp_PrActGivenComp.mgz\n",
    "        │   │   ├── lh.Yeo_Brainmap_10to14Comp_Flexibility.mgz\n",
    "        │   │   ├── lh.Yeo_Brainmap_10to14Comp_Specialization.mgz\n",
    "        │   │   ├── lh.Yeo_Brainmap_10to14Comp_SpecializationROI.mgz\n",
    "        │   │   ├── lh.Yeo_Brainmap_10to14Comp_TopSpecializationComp.csv\n",
    "        │   │   ├── lh.Yeo_Brainmap_11Comp_PrActGivenComp.mgz\n",
    "        │   │   ├── lh.Yeo_Brainmap_12Comp_PrActGivenComp.mgz\n",
    "        │   │   ├── lh.Yeo_Brainmap_13Comp_PrActGivenComp.mgz\n",
    "        │   │   ├── lh.Yeo_Brainmap_14Comp_PrActGivenComp.mgz\n",
    "        │   │   ├── lh.aparc.a2005s.annot\n",
    "        │   │   ├── lh.aparc.a2009s.annot\n",
    "        │   │   ├── lh.aparc.annot\n",
    "        │   │   ├── lh.aparc.label\n",
    "        │   │   ├── lh.cortex.label\n",
    "        │   │   ├── lh.entorhinal_exvivo.label\n",
    "        │   │   ├── lh.entorhinal_exvivo.thresh.label\n",
    "        │   │   ├── lh.oasis.chubs.annot\n",
    "        │   │   ├── lh.oasis.chubs.ifc.label\n",
    "        │   │   ├── lh.oasis.chubs.ipc.label\n",
    "        │   │   ├── lh.oasis.chubs.ips.label\n",
    "        │   │   ├── lh.oasis.chubs.lateraltemporal.label\n",
    "        │   │   ├── lh.oasis.chubs.medialpfc.label\n",
    "        │   │   ├── lh.oasis.chubs.mtl.label\n",
    "        │   │   ├── lh.oasis.chubs.retrosplenial.label\n",
    "        │   │   ├── lh.oasis.chubs.tp.label\n",
    "        │   │   ├── lh.perirhinal_exvivo.label\n",
    "        │   │   ├── lh.perirhinal_exvivo.thresh.label\n",
    "        │   │   ├── rh.BA1_exvivo.label\n",
    "        │   │   ├── rh.BA1_exvivo.thresh.label\n",
    "        │   │   ├── rh.BA2_exvivo.label\n",
    "        │   │   ├── rh.BA2_exvivo.thresh.label\n",
    "        │   │   ├── rh.BA3a_exvivo.label\n",
    "        │   │   ├── rh.BA3a_exvivo.thresh.label\n",
    "        │   │   ├── rh.BA3b_exvivo.label\n",
    "        │   │   ├── rh.BA3b_exvivo.thresh.label\n",
    "        │   │   ├── rh.BA44_exvivo.label\n",
    "        │   │   ├── rh.BA44_exvivo.thresh.label\n",
    "        │   │   ├── rh.BA45_exvivo.label\n",
    "        │   │   ├── rh.BA45_exvivo.thresh.label\n",
    "        │   │   ├── rh.BA4a_exvivo.label\n",
    "        │   │   ├── rh.BA4a_exvivo.thresh.label\n",
    "        │   │   ├── rh.BA4p_exvivo.label\n",
    "        │   │   ├── rh.BA4p_exvivo.thresh.label\n",
    "        │   │   ├── rh.BA6_exvivo.label\n",
    "        │   │   ├── rh.BA6_exvivo.thresh.label\n",
    "        │   │   ├── rh.MT_exvivo.label\n",
    "        │   │   ├── rh.MT_exvivo.thresh.label\n",
    "        │   │   ├── rh.Medial_wall.label\n",
    "        │   │   ├── rh.PALS_B12.labels.gii\n",
    "        │   │   ├── rh.PALS_B12_Brodmann.annot\n",
    "        │   │   ├── rh.PALS_B12_Lobes.annot\n",
    "        │   │   ├── rh.PALS_B12_OrbitoFrontal.annot\n",
    "        │   │   ├── rh.PALS_B12_Visuotopic.annot\n",
    "        │   │   ├── rh.V1_exvivo.label\n",
    "        │   │   ├── rh.V1_exvivo.thresh.label\n",
    "        │   │   ├── rh.V2_exvivo.label\n",
    "        │   │   ├── rh.V2_exvivo.thresh.label\n",
    "        │   │   ├── rh.Yeo2011_17NetworksConfidence_N1000.mgz\n",
    "        │   │   ├── rh.Yeo2011_17Networks_N1000.annot\n",
    "        │   │   ├── rh.Yeo2011_7NetworksConfidence_N1000.mgz\n",
    "        │   │   ├── rh.Yeo2011_7Networks_N1000.annot\n",
    "        │   │   ├── rh.Yeo_Brainmap_10Comp_PrActGivenComp.mgz\n",
    "        │   │   ├── rh.Yeo_Brainmap_10to14Comp_Flexibility.mgz\n",
    "        │   │   ├── rh.Yeo_Brainmap_10to14Comp_Specialization.mgz\n",
    "        │   │   ├── rh.Yeo_Brainmap_10to14Comp_SpecializationROI.mgz\n",
    "        │   │   ├── rh.Yeo_Brainmap_10to14Comp_TopSpecializationComp.csv\n",
    "        │   │   ├── rh.Yeo_Brainmap_11Comp_PrActGivenComp.mgz\n",
    "        │   │   ├── rh.Yeo_Brainmap_12Comp_PrActGivenComp.mgz\n",
    "        │   │   ├── rh.Yeo_Brainmap_13Comp_PrActGivenComp.mgz\n",
    "        │   │   ├── rh.Yeo_Brainmap_14Comp_PrActGivenComp.mgz\n",
    "        │   │   ├── rh.aparc.a2005s.annot\n",
    "        │   │   ├── rh.aparc.a2009s.annot\n",
    "        │   │   ├── rh.aparc.annot\n",
    "        │   │   ├── rh.aparc.label\n",
    "        │   │   ├── rh.cortex.label\n",
    "        │   │   ├── rh.entorhinal_exvivo.label\n",
    "        │   │   ├── rh.entorhinal_exvivo.thresh.label\n",
    "        │   │   ├── rh.oasis.chubs.annot\n",
    "        │   │   ├── rh.oasis.chubs.ifc.label\n",
    "        │   │   ├── rh.oasis.chubs.ipc.label\n",
    "        │   │   ├── rh.oasis.chubs.ips.label\n",
    "        │   │   ├── rh.oasis.chubs.lateraltemporal.label\n",
    "        │   │   ├── rh.oasis.chubs.medialpfc.label\n",
    "        │   │   ├── rh.oasis.chubs.mtl.label\n",
    "        │   │   ├── rh.oasis.chubs.retrosplenial.label\n",
    "        │   │   ├── rh.oasis.chubs.tp.label\n",
    "        │   │   ├── rh.perirhinal_exvivo.label\n",
    "        │   │   └── rh.perirhinal_exvivo.thresh.label\n",
    "        │   ├── mri\n",
    "        │   │   ├── T1.mgz\n",
    "        │   │   ├── aparc+aseg.mgz\n",
    "        │   │   ├── aparc.a2005s+aseg.mgz\n",
    "        │   │   ├── aparc.a2009s+aseg.mgz\n",
    "        │   │   ├── aseg.mgz\n",
    "        │   │   ├── brain.mgz\n",
    "        │   │   ├── brainmask.mgz\n",
    "        │   │   ├── lh.ribbon.mgz\n",
    "        │   │   ├── mni305.cor.mgz\n",
    "        │   │   ├── orig\n",
    "        │   │   ├── orig.mgz\n",
    "        │   │   ├── p.aseg.mgz\n",
    "        │   │   ├── rh.ribbon.mgz\n",
    "        │   │   ├── ribbon.mgz\n",
    "        │   │   ├── subcort.mask.1mm.README\n",
    "        │   │   ├── subcort.mask.1mm.mgz\n",
    "        │   │   ├── subcort.prob.log\n",
    "        │   │   ├── subcort.prob.mgz\n",
    "        │   │   └── transforms\n",
    "        │   │       ├── bak\n",
    "        │   │       ├── reg.mni152.2mm.dat\n",
    "        │   │       └── talairach.xfm\n",
    "        │   ├── mri.2mm\n",
    "        │   │   ├── README\n",
    "        │   │   ├── T1.mgz\n",
    "        │   │   ├── aseg.mgz\n",
    "        │   │   ├── brain.mgz\n",
    "        │   │   ├── brainmask.mgz\n",
    "        │   │   ├── mni305.cor.mgz\n",
    "        │   │   ├── orig.mgz\n",
    "        │   │   ├── reg.2mm.dat\n",
    "        │   │   ├── reg.2mm.mni152.dat\n",
    "        │   │   ├── subcort.mask.mgz\n",
    "        │   │   └── subcort.prob.mgz\n",
    "        │   ├── scripts\n",
    "        │   │   ├── build-stamp.txt\n",
    "        │   │   ├── cvs_log_pre_31May2011.txt\n",
    "        │   │   ├── make_average_surface.log\n",
    "        │   │   ├── make_average_volume.log\n",
    "        │   │   ├── mris_inflate.log\n",
    "        │   │   ├── mris_inflate_lh.log\n",
    "        │   │   ├── mris_inflate_rh.log\n",
    "        │   │   ├── recon-all-status.log\n",
    "        │   │   ├── recon-all.cmd\n",
    "        │   │   ├── recon-all.done\n",
    "        │   │   ├── recon-all.env\n",
    "        │   │   ├── recon-all.env.bak\n",
    "        │   │   ├── recon-all.local-copy\n",
    "        │   │   ├── recon-all.log\n",
    "        │   │   ├── surfreg.fsaverage_sym.lh.log\n",
    "        │   │   └── surfreg.fsaverage_sym.rh.log\n",
    "        │   ├── surf\n",
    "        │   │   ├── lh.area\n",
    "        │   │   ├── lh.avg_curv\n",
    "        │   │   ├── lh.avg_sulc\n",
    "        │   │   ├── lh.avg_thickness\n",
    "        │   │   ├── lh.cortex.patch.3d\n",
    "        │   │   ├── lh.cortex.patch.flat\n",
    "        │   │   ├── lh.curv\n",
    "        │   │   ├── lh.fsaverage_sym.sphere.reg\n",
    "        │   │   ├── lh.inflated\n",
    "        │   │   ├── lh.inflated.H\n",
    "        │   │   ├── lh.inflated.K\n",
    "        │   │   ├── lh.inflated_avg\n",
    "        │   │   ├── lh.inflated_pre\n",
    "        │   │   ├── lh.orig\n",
    "        │   │   ├── lh.orig.avg.area.mgh\n",
    "        │   │   ├── lh.orig_avg\n",
    "        │   │   ├── lh.pial\n",
    "        │   │   ├── lh.pial.avg.area.mgh\n",
    "        │   │   ├── lh.pial_avg\n",
    "        │   │   ├── lh.pial_semi_inflated\n",
    "        │   │   ├── lh.smoothwm\n",
    "        │   │   ├── lh.sphere\n",
    "        │   │   ├── lh.sphere.left_right\n",
    "        │   │   ├── lh.sphere.reg\n",
    "        │   │   ├── lh.sphere.reg.avg\n",
    "        │   │   ├── lh.sulc\n",
    "        │   │   ├── lh.thickness\n",
    "        │   │   ├── lh.white\n",
    "        │   │   ├── lh.white.avg.area.mgh\n",
    "        │   │   ├── lh.white_avg\n",
    "        │   │   ├── lh.white_avg.H\n",
    "        │   │   ├── lh.white_avg.K\n",
    "        │   │   ├── mris_preproc.surface.lh.log\n",
    "        │   │   ├── mris_preproc.surface.rh.log\n",
    "        │   │   ├── rh.area\n",
    "        │   │   ├── rh.avg_curv\n",
    "        │   │   ├── rh.avg_sulc\n",
    "        │   │   ├── rh.avg_thickness\n",
    "        │   │   ├── rh.cortex.patch.3d\n",
    "        │   │   ├── rh.cortex.patch.flat\n",
    "        │   │   ├── rh.curv\n",
    "        │   │   ├── rh.fsaverage_sym.sphere.reg\n",
    "        │   │   ├── rh.inflated\n",
    "        │   │   ├── rh.inflated.H\n",
    "        │   │   ├── rh.inflated.K\n",
    "        │   │   ├── rh.inflated_avg\n",
    "        │   │   ├── rh.inflated_pre\n",
    "        │   │   ├── rh.orig\n",
    "        │   │   ├── rh.orig.avg.area.mgh\n",
    "        │   │   ├── rh.orig_avg\n",
    "        │   │   ├── rh.pial\n",
    "        │   │   ├── rh.pial.avg.area.mgh\n",
    "        │   │   ├── rh.pial_avg\n",
    "        │   │   ├── rh.pial_semi_inflated\n",
    "        │   │   ├── rh.smoothwm\n",
    "        │   │   ├── rh.sphere\n",
    "        │   │   ├── rh.sphere.left_right\n",
    "        │   │   ├── rh.sphere.reg\n",
    "        │   │   ├── rh.sphere.reg.avg\n",
    "        │   │   ├── rh.sulc\n",
    "        │   │   ├── rh.thickness\n",
    "        │   │   ├── rh.white\n",
    "        │   │   ├── rh.white.avg.area.mgh\n",
    "        │   │   ├── rh.white_avg\n",
    "        │   │   ├── rh.white_avg.H\n",
    "        │   │   └── rh.white_avg.K\n",
    "        │   └── xhemi\n",
    "        │       ├── bem\n",
    "        │       ├── label\n",
    "        │       │   ├── lh.aparc.a2009s.annot\n",
    "        │       │   ├── lh.aparc.annot\n",
    "        │       │   ├── lh.cortex.label\n",
    "        │       │   ├── rh.aparc.a2009s.annot\n",
    "        │       │   ├── rh.aparc.annot\n",
    "        │       │   └── rh.cortex.label\n",
    "        │       ├── lrrev.pure.register.dat\n",
    "        │       ├── lrrev.register.dat\n",
    "        │       ├── mri\n",
    "        │       │   ├── T1.mgz\n",
    "        │       │   ├── aparc+aseg.mgz\n",
    "        │       │   ├── aseg.mgz\n",
    "        │       │   ├── brain.mgz\n",
    "        │       │   ├── brainmask.mgz\n",
    "        │       │   ├── mri_nu_correct.mni.log\n",
    "        │       │   ├── orig\n",
    "        │       │   ├── orig.mgz\n",
    "        │       │   ├── orig_nu.mgz\n",
    "        │       │   └── transforms\n",
    "        │       │       ├── bak\n",
    "        │       │       ├── talairach.auto.xfm\n",
    "        │       │       ├── talairach.xfm\n",
    "        │       │       ├── talairach_avi.log\n",
    "        │       │       └── talsrcimg_to_711-2C_as_mni_average_305_t4_vox2vox.txt\n",
    "        │       ├── scripts\n",
    "        │       │   ├── build-stamp.txt\n",
    "        │       │   ├── lastcall.build-stamp.txt\n",
    "        │       │   ├── patchdir.txt\n",
    "        │       │   ├── recon-all-status.log\n",
    "        │       │   ├── recon-all.cmd\n",
    "        │       │   ├── recon-all.done\n",
    "        │       │   ├── recon-all.env\n",
    "        │       │   ├── recon-all.local-copy\n",
    "        │       │   ├── recon-all.log\n",
    "        │       │   ├── surfreg.fsaverage_sym.lh.log\n",
    "        │       │   └── surfreg.fsaverage_sym.rh.log\n",
    "        │       ├── src\n",
    "        │       ├── stats\n",
    "        │       ├── surf\n",
    "        │       │   ├── lh.area\n",
    "        │       │   ├── lh.curv\n",
    "        │       │   ├── lh.fsaverage_sym.sphere.reg\n",
    "        │       │   ├── lh.inflated\n",
    "        │       │   ├── lh.inflated.H\n",
    "        │       │   ├── lh.inflated.K\n",
    "        │       │   ├── lh.orig\n",
    "        │       │   ├── lh.pial\n",
    "        │       │   ├── lh.smoothwm\n",
    "        │       │   ├── lh.sphere\n",
    "        │       │   ├── lh.sulc\n",
    "        │       │   ├── lh.thickness\n",
    "        │       │   ├── lh.white\n",
    "        │       │   ├── rh.area\n",
    "        │       │   ├── rh.curv\n",
    "        │       │   ├── rh.fsaverage_sym.sphere.reg\n",
    "        │       │   ├── rh.inflated\n",
    "        │       │   ├── rh.inflated.H\n",
    "        │       │   ├── rh.inflated.K\n",
    "        │       │   ├── rh.orig\n",
    "        │       │   ├── rh.pial\n",
    "        │       │   ├── rh.smoothwm\n",
    "        │       │   ├── rh.sphere\n",
    "        │       │   ├── rh.sulc\n",
    "        │       │   ├── rh.thickness\n",
    "        │       │   └── rh.white\n",
    "        │       ├── tmp\n",
    "        │       ├── touch\n",
    "        │       │   └── talairach.touch\n",
    "        │       ├── trash\n",
    "        │       ├── xhemireg.lh.log\n",
    "        │       └── xhemireg.rh.log\n",
    "        └── sub-00001\n",
    "            ├── label\n",
    "            │   ├── BA_exvivo.ctab\n",
    "            │   ├── BA_exvivo.thresh.ctab\n",
    "            │   ├── aparc.annot.DKTatlas.ctab\n",
    "            │   ├── aparc.annot.a2009s.ctab\n",
    "            │   ├── aparc.annot.ctab\n",
    "            │   ├── lh.BA1_exvivo.label\n",
    "            │   ├── lh.BA1_exvivo.thresh.label\n",
    "            │   ├── lh.BA2_exvivo.label\n",
    "            │   ├── lh.BA2_exvivo.thresh.label\n",
    "            │   ├── lh.BA3a_exvivo.label\n",
    "            │   ├── lh.BA3a_exvivo.thresh.label\n",
    "            │   ├── lh.BA3b_exvivo.label\n",
    "            │   ├── lh.BA3b_exvivo.thresh.label\n",
    "            │   ├── lh.BA44_exvivo.label\n",
    "            │   ├── lh.BA44_exvivo.thresh.label\n",
    "            │   ├── lh.BA45_exvivo.label\n",
    "            │   ├── lh.BA45_exvivo.thresh.label\n",
    "            │   ├── lh.BA4a_exvivo.label\n",
    "            │   ├── lh.BA4a_exvivo.thresh.label\n",
    "            │   ├── lh.BA4p_exvivo.label\n",
    "            │   ├── lh.BA4p_exvivo.thresh.label\n",
    "            │   ├── lh.BA6_exvivo.label\n",
    "            │   ├── lh.BA6_exvivo.thresh.label\n",
    "            │   ├── lh.BA_exvivo.annot\n",
    "            │   ├── lh.BA_exvivo.thresh.annot\n",
    "            │   ├── lh.MT_exvivo.label\n",
    "            │   ├── lh.MT_exvivo.thresh.label\n",
    "            │   ├── lh.V1_exvivo.label\n",
    "            │   ├── lh.V1_exvivo.thresh.label\n",
    "            │   ├── lh.V2_exvivo.label\n",
    "            │   ├── lh.V2_exvivo.thresh.label\n",
    "            │   ├── lh.aparc.DKTatlas.annot\n",
    "            │   ├── lh.aparc.a2009s.annot\n",
    "            │   ├── lh.aparc.annot\n",
    "            │   ├── lh.cortex.label\n",
    "            │   ├── lh.entorhinal_exvivo.label\n",
    "            │   ├── lh.entorhinal_exvivo.thresh.label\n",
    "            │   ├── lh.perirhinal_exvivo.label\n",
    "            │   ├── lh.perirhinal_exvivo.thresh.label\n",
    "            │   ├── rh.BA1_exvivo.label\n",
    "            │   ├── rh.BA1_exvivo.thresh.label\n",
    "            │   ├── rh.BA2_exvivo.label\n",
    "            │   ├── rh.BA2_exvivo.thresh.label\n",
    "            │   ├── rh.BA3a_exvivo.label\n",
    "            │   ├── rh.BA3a_exvivo.thresh.label\n",
    "            │   ├── rh.BA3b_exvivo.label\n",
    "            │   ├── rh.BA3b_exvivo.thresh.label\n",
    "            │   ├── rh.BA44_exvivo.label\n",
    "            │   ├── rh.BA44_exvivo.thresh.label\n",
    "            │   ├── rh.BA45_exvivo.label\n",
    "            │   ├── rh.BA45_exvivo.thresh.label\n",
    "            │   ├── rh.BA4a_exvivo.label\n",
    "            │   ├── rh.BA4a_exvivo.thresh.label\n",
    "            │   ├── rh.BA4p_exvivo.label\n",
    "            │   ├── rh.BA4p_exvivo.thresh.label\n",
    "            │   ├── rh.BA6_exvivo.label\n",
    "            │   ├── rh.BA6_exvivo.thresh.label\n",
    "            │   ├── rh.BA_exvivo.annot\n",
    "            │   ├── rh.BA_exvivo.thresh.annot\n",
    "            │   ├── rh.MT_exvivo.label\n",
    "            │   ├── rh.MT_exvivo.thresh.label\n",
    "            │   ├── rh.V1_exvivo.label\n",
    "            │   ├── rh.V1_exvivo.thresh.label\n",
    "            │   ├── rh.V2_exvivo.label\n",
    "            │   ├── rh.V2_exvivo.thresh.label\n",
    "            │   ├── rh.aparc.DKTatlas.annot\n",
    "            │   ├── rh.aparc.a2009s.annot\n",
    "            │   ├── rh.aparc.annot\n",
    "            │   ├── rh.cortex.label\n",
    "            │   ├── rh.entorhinal_exvivo.label\n",
    "            │   ├── rh.entorhinal_exvivo.thresh.label\n",
    "            │   ├── rh.perirhinal_exvivo.label\n",
    "            │   └── rh.perirhinal_exvivo.thresh.label\n",
    "            ├── mri\n",
    "            │   ├── T1.mgz\n",
    "            │   ├── aparc+aseg.mgz\n",
    "            │   ├── aparc.DKTatlas+aseg.mgz\n",
    "            │   ├── aparc.a2009s+aseg.mgz\n",
    "            │   ├── aseg.auto.mgz\n",
    "            │   ├── aseg.auto_noCCseg.label_intensities.txt\n",
    "            │   ├── aseg.auto_noCCseg.mgz\n",
    "            │   ├── aseg.mgz\n",
    "            │   ├── aseg.presurf.hypos.mgz\n",
    "            │   ├── aseg.presurf.mgz\n",
    "            │   ├── brain.finalsurfs.mgz\n",
    "            │   ├── brain.mgz\n",
    "            │   ├── brainmask.auto.mgz\n",
    "            │   ├── brainmask.mgz\n",
    "            │   ├── ctrl_pts.mgz\n",
    "            │   ├── filled.mgz\n",
    "            │   ├── lh.ribbon.mgz\n",
    "            │   ├── mri_nu_correct.mni.log\n",
    "            │   ├── mri_nu_correct.mni.log.bak\n",
    "            │   ├── norm.mgz\n",
    "            │   ├── nu.mgz\n",
    "            │   ├── orig\n",
    "            │   │   └── 001.mgz\n",
    "            │   ├── orig.mgz\n",
    "            │   ├── orig_nu.mgz\n",
    "            │   ├── rawavg.mgz\n",
    "            │   ├── rh.ribbon.mgz\n",
    "            │   ├── ribbon.mgz\n",
    "            │   ├── segment.dat\n",
    "            │   ├── talairach.label_intensities.txt\n",
    "            │   ├── talairach.log\n",
    "            │   ├── transforms\n",
    "            │   │   ├── bak\n",
    "            │   │   ├── cc_up.lta\n",
    "            │   │   ├── talairach.auto.xfm\n",
    "            │   │   ├── talairach.auto.xfm.lta\n",
    "            │   │   ├── talairach.lta\n",
    "            │   │   ├── talairach.m3z\n",
    "            │   │   ├── talairach.xfm\n",
    "            │   │   ├── talairach_avi.log\n",
    "            │   │   ├── talairach_avi_QA.log\n",
    "            │   │   └── talsrcimg_to_711-2C_as_mni_average_305_t4_vox2vox.txt\n",
    "            │   ├── wm.asegedit.mgz\n",
    "            │   ├── wm.mgz\n",
    "            │   ├── wm.seg.mgz\n",
    "            │   └── wmparc.mgz\n",
    "            ├── scripts\n",
    "            │   ├── build-stamp.txt\n",
    "            │   ├── lastcall.build-stamp.txt\n",
    "            │   ├── patchdir.txt\n",
    "            │   ├── pctsurfcon.log\n",
    "            │   ├── pctsurfcon.log.old\n",
    "            │   ├── ponscc.cut.log\n",
    "            │   ├── recon-all-lh.cmd\n",
    "            │   ├── recon-all-lh.log\n",
    "            │   ├── recon-all-rh.cmd\n",
    "            │   ├── recon-all-rh.log\n",
    "            │   ├── recon-all-status-lh.log\n",
    "            │   ├── recon-all-status-rh.log\n",
    "            │   ├── recon-all-status.log\n",
    "            │   ├── recon-all.cmd\n",
    "            │   ├── recon-all.done\n",
    "            │   ├── recon-all.env\n",
    "            │   ├── recon-all.env.bak\n",
    "            │   ├── recon-all.local-copy\n",
    "            │   └── recon-all.log\n",
    "            ├── stats\n",
    "            │   ├── aseg.stats\n",
    "            │   ├── lh.BA_exvivo.stats\n",
    "            │   ├── lh.BA_exvivo.thresh.stats\n",
    "            │   ├── lh.aparc.DKTatlas.stats\n",
    "            │   ├── lh.aparc.a2009s.stats\n",
    "            │   ├── lh.aparc.pial.stats\n",
    "            │   ├── lh.aparc.stats\n",
    "            │   ├── lh.curv.stats\n",
    "            │   ├── lh.w-g.pct.stats\n",
    "            │   ├── rh.BA_exvivo.stats\n",
    "            │   ├── rh.BA_exvivo.thresh.stats\n",
    "            │   ├── rh.aparc.DKTatlas.stats\n",
    "            │   ├── rh.aparc.a2009s.stats\n",
    "            │   ├── rh.aparc.pial.stats\n",
    "            │   ├── rh.aparc.stats\n",
    "            │   ├── rh.curv.stats\n",
    "            │   ├── rh.w-g.pct.stats\n",
    "            │   └── wmparc.stats\n",
    "            ├── surf\n",
    "            │   ├── lh.area\n",
    "            │   ├── lh.area.mid\n",
    "            │   ├── lh.area.pial\n",
    "            │   ├── lh.avg_curv\n",
    "            │   ├── lh.curv\n",
    "            │   ├── lh.curv.pial\n",
    "            │   ├── lh.defect_borders\n",
    "            │   ├── lh.defect_chull\n",
    "            │   ├── lh.defect_labels\n",
    "            │   ├── lh.inflated\n",
    "            │   ├── lh.inflated.H\n",
    "            │   ├── lh.inflated.K\n",
    "            │   ├── lh.inflated.nofix\n",
    "            │   ├── lh.jacobian_white\n",
    "            │   ├── lh.midthickness\n",
    "            │   ├── lh.orig\n",
    "            │   ├── lh.orig.nofix\n",
    "            │   ├── lh.pial\n",
    "            │   ├── lh.qsphere.nofix\n",
    "            │   ├── lh.smoothwm\n",
    "            │   ├── lh.smoothwm.BE.crv\n",
    "            │   ├── lh.smoothwm.C.crv\n",
    "            │   ├── lh.smoothwm.FI.crv\n",
    "            │   ├── lh.smoothwm.H.crv\n",
    "            │   ├── lh.smoothwm.K.crv\n",
    "            │   ├── lh.smoothwm.K1.crv\n",
    "            │   ├── lh.smoothwm.K2.crv\n",
    "            │   ├── lh.smoothwm.S.crv\n",
    "            │   ├── lh.smoothwm.nofix\n",
    "            │   ├── lh.sphere\n",
    "            │   ├── lh.sphere.reg\n",
    "            │   ├── lh.sulc\n",
    "            │   ├── lh.thickness\n",
    "            │   ├── lh.volume\n",
    "            │   ├── lh.w-g.pct.mgh\n",
    "            │   ├── lh.white\n",
    "            │   ├── lh.white.H -> lh.white.preaparc.H\n",
    "            │   ├── lh.white.K -> lh.white.preaparc.K\n",
    "            │   ├── lh.white.preaparc\n",
    "            │   ├── lh.white.preaparc.H\n",
    "            │   ├── lh.white.preaparc.K\n",
    "            │   ├── rh.area\n",
    "            │   ├── rh.area.mid\n",
    "            │   ├── rh.area.pial\n",
    "            │   ├── rh.avg_curv\n",
    "            │   ├── rh.curv\n",
    "            │   ├── rh.curv.pial\n",
    "            │   ├── rh.defect_borders\n",
    "            │   ├── rh.defect_chull\n",
    "            │   ├── rh.defect_labels\n",
    "            │   ├── rh.inflated\n",
    "            │   ├── rh.inflated.H\n",
    "            │   ├── rh.inflated.K\n",
    "            │   ├── rh.inflated.nofix\n",
    "            │   ├── rh.jacobian_white\n",
    "            │   ├── rh.midthickness\n",
    "            │   ├── rh.orig\n",
    "            │   ├── rh.orig.nofix\n",
    "            │   ├── rh.pial\n",
    "            │   ├── rh.qsphere.nofix\n",
    "            │   ├── rh.smoothwm\n",
    "            │   ├── rh.smoothwm.BE.crv\n",
    "            │   ├── rh.smoothwm.C.crv\n",
    "            │   ├── rh.smoothwm.FI.crv\n",
    "            │   ├── rh.smoothwm.H.crv\n",
    "            │   ├── rh.smoothwm.K.crv\n",
    "            │   ├── rh.smoothwm.K1.crv\n",
    "            │   ├── rh.smoothwm.K2.crv\n",
    "            │   ├── rh.smoothwm.S.crv\n",
    "            │   ├── rh.smoothwm.nofix\n",
    "            │   ├── rh.sphere\n",
    "            │   ├── rh.sphere.reg\n",
    "            │   ├── rh.sulc\n",
    "            │   ├── rh.thickness\n",
    "            │   ├── rh.volume\n",
    "            │   ├── rh.w-g.pct.mgh\n",
    "            │   ├── rh.white\n",
    "            │   ├── rh.white.H -> rh.white.preaparc.H\n",
    "            │   ├── rh.white.K -> rh.white.preaparc.K\n",
    "            │   ├── rh.white.preaparc\n",
    "            │   ├── rh.white.preaparc.H\n",
    "            │   └── rh.white.preaparc.K\n",
    "            ├── tmp\n",
    "            ├── touch\n",
    "            │   ├── aparc.DKTatlas2aseg.touch\n",
    "            │   ├── aparc.a2009s2aseg.touch\n",
    "            │   ├── apas2aseg.touch\n",
    "            │   ├── asegmerge.touch\n",
    "            │   ├── ca_label.touch\n",
    "            │   ├── ca_normalize.touch\n",
    "            │   ├── ca_register.touch\n",
    "            │   ├── conform.touch\n",
    "            │   ├── cortical_ribbon.touch\n",
    "            │   ├── em_register.touch\n",
    "            │   ├── fill.touch\n",
    "            │   ├── inorm1.touch\n",
    "            │   ├── inorm2.touch\n",
    "            │   ├── lh.aparc.touch\n",
    "            │   ├── lh.aparc2.touch\n",
    "            │   ├── lh.aparcstats.touch\n",
    "            │   ├── lh.aparcstats2.touch\n",
    "            │   ├── lh.aparcstats3.touch\n",
    "            │   ├── lh.avgcurv.touch\n",
    "            │   ├── lh.curvstats.touch\n",
    "            │   ├── lh.final_surfaces.touch\n",
    "            │   ├── lh.inflate.H.K.touch\n",
    "            │   ├── lh.inflate1.touch\n",
    "            │   ├── lh.inflate2.touch\n",
    "            │   ├── lh.jacobian_white.touch\n",
    "            │   ├── lh.pctsurfcon.touch\n",
    "            │   ├── lh.pial_surface.touch\n",
    "            │   ├── lh.qsphere.touch\n",
    "            │   ├── lh.smoothwm1.touch\n",
    "            │   ├── lh.smoothwm2.touch\n",
    "            │   ├── lh.sphmorph.touch\n",
    "            │   ├── lh.sphreg.touch\n",
    "            │   ├── lh.surfvolume.touch\n",
    "            │   ├── lh.tessellate.touch\n",
    "            │   ├── lh.topofix.touch\n",
    "            │   ├── lh.white.H.K.touch\n",
    "            │   ├── lh.white_surface.touch\n",
    "            │   ├── nu.touch\n",
    "            │   ├── relabelhypos.touch\n",
    "            │   ├── rh.aparc.touch\n",
    "            │   ├── rh.aparc2.touch\n",
    "            │   ├── rh.aparcstats.touch\n",
    "            │   ├── rh.aparcstats2.touch\n",
    "            │   ├── rh.aparcstats3.touch\n",
    "            │   ├── rh.avgcurv.touch\n",
    "            │   ├── rh.curvstats.touch\n",
    "            │   ├── rh.final_surfaces.touch\n",
    "            │   ├── rh.inflate.H.K.touch\n",
    "            │   ├── rh.inflate1.touch\n",
    "            │   ├── rh.inflate2.touch\n",
    "            │   ├── rh.jacobian_white.touch\n",
    "            │   ├── rh.pctsurfcon.touch\n",
    "            │   ├── rh.pial_surface.touch\n",
    "            │   ├── rh.qsphere.touch\n",
    "            │   ├── rh.smoothwm1.touch\n",
    "            │   ├── rh.smoothwm2.touch\n",
    "            │   ├── rh.sphmorph.touch\n",
    "            │   ├── rh.sphreg.touch\n",
    "            │   ├── rh.surfvolume.touch\n",
    "            │   ├── rh.tessellate.touch\n",
    "            │   ├── rh.topofix.touch\n",
    "            │   ├── rh.white.H.K.touch\n",
    "            │   ├── rh.white_surface.touch\n",
    "            │   ├── rusage.mri_ca_register.dat\n",
    "            │   ├── rusage.mri_em_register.dat\n",
    "            │   ├── rusage.mris_fix_topology.lh.dat\n",
    "            │   ├── rusage.mris_fix_topology.rh.dat\n",
    "            │   ├── rusage.mris_inflate.lh.dat\n",
    "            │   ├── rusage.mris_inflate.rh.dat\n",
    "            │   ├── rusage.mris_register.lh.dat\n",
    "            │   ├── rusage.mris_register.rh.dat\n",
    "            │   ├── rusage.mris_sphere.lh.dat\n",
    "            │   ├── rusage.mris_sphere.rh.dat\n",
    "            │   ├── segstats.touch\n",
    "            │   ├── talairach.touch\n",
    "            │   ├── wmaparc.stats.touch\n",
    "            │   ├── wmaparc.touch\n",
    "            │   └── wmsegment.touch\n",
    "            └── trash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! That is SO many files. So what do we need to keep? That depends on what you want to do. \n",
    "\n",
    "In our case, we want to extract functional connectivity matrices from the bold images. We also want the pre-processed T1w image so that we can plot the brain activity on a nice anatomical image. Depending on the study/plotting decision, you can definately use the Freesurfer outputs. Please note that, by default, the different MRI **are merged together in the fMRIPrep pipeline to allow for co-registration of BOLD signal on an averaged structural image**. To my knowledge, and according to the app's developper, [there is not currently a way to signal fMRIPrep to output FreeSurfer outputs for each session](https://github.com/poldracklab/fmriprep/issues/993). Their goal was more to provide a strong registration for the BOLD signal, not to allow analyses using structural variables. As such, if the FreeSurfer outputs for each visits are of capital importance, then either:\n",
    "\n",
    "\n",
    "1) Pre-processing with fMRIPrep should be run for 1 session at a time. In such case, the registration and Freesurfer output is given for a single visit. At this time however, I am not sure if this solution works and it would need to be tried.  \n",
    "2) FreeSurfer can be run separately before fMRIPrep and fMRIPrep can reuse the outputs. In that case, FreeSurfer outputs 6.0.0 needs to be used to be recognized by fMRIPrep and the FreeSurfer outputs need to be in the same output folder as fMRIPrep output will be in.\n",
    "\n",
    "So ultimately, we will use the files below in the pipeline/or these files are useful:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "derivatives/\n",
    "└── fmriprep\n",
    "    ├── fmriprep\n",
    "    │   ├── dataset_description.json\n",
    "    │   ├── logs\n",
    "    │   │   └── CITATION.md\n",
    "    │   ├── sub-00001\n",
    "    │   │   ├── anat\n",
    "    │   │   │   ├── sub-00001_desc-preproc_T1w.json\n",
    "    │   │   │   └── sub-00001_desc-preproc_T1w.nii.gz\n",
    "    │   │   ├── ses-BL00A\n",
    "    │   │   │   └── func\n",
    "    │   │   │       ├── sub-00001_ses-BL00A_task-rest_run-1_desc-confounds_regressors.json\n",
    "    │   │   │       ├── sub-00001_ses-BL00A_task-rest_run-1_desc-confounds_regressors.tsv\n",
    "    │   │   │       ├── sub-00001_ses-BL00A_task-rest_run-1_space-T1w_desc-preproc_bold.json\n",
    "    │   │   │       └── sub-00001_ses-BL00A_task-rest_run-1_space-T1w_desc-preproc_bold.nii.gz\n",
    "    │   │   └── ses-FU12A\n",
    "    │   │       └── func\n",
    "    │   │           ├── sub-00001_ses-FU12A_task-rest_run-1_desc-confounds_regressors.json\n",
    "    │   │           ├── sub-00001_ses-FU12A_task-rest_run-1_desc-confounds_regressors.tsv\n",
    "    │   │           ├── sub-00001_ses-FU12A_task-rest_run-1_space-T1w_desc-preproc_bold.json\n",
    "    │   │           └── sub-00001_ses-FU12A_task-rest_run-1_space-T1w_desc-preproc_bold.nii.gz\n",
    "    │   └── sub-00001.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```dataset_description.json```**: Gives the the version of the bids-validator used, the version of fMRIPrep used and how to acknowledge the pipeline in a paper.\n",
    "\n",
    "**```CITATION.md```**: fMRIPrep generates an exact description of the steps included in the pre-processing and is intended to be copy-pasted in a scientific article using fMRIPrep. Note that depending on how you ran the pre-processing, this citation will change. \n",
    "\n",
    "**```anat```**: This folder contains the 'averaged' MRI files. In our case, we want the 'desc-preproc' files.\n",
    "\n",
    "**```func```**: These folders are generated for each session used. They include many files, but of interest, we need the ```.tsv``` file containing the fMRI confounds for each subject at each session. We also need the fMRI file with 'desc-preproc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Connectivity matrices extraction\n",
    "\n",
    "As a disclaimer, the following Nilearn pipeline is not complete and does not yet scrub time series for volumes with high frame displacement. This will soon be added. The script used to extract matrices is detailed here, but is also available in GitHub as a standalone script.\n",
    "\n",
    "The goal of this section is to take the pre-processed images, extract the time series information using a defined brain parcellation, and regress the confounds out of the connectivity matrices. The script then saves the connectivity matrices as .csv files that be used for later analyses and as .png images that can be used for summary QC/visualization.\n",
    "\n",
    "This script is based on this [Nilearn Example](https://nilearn.github.io/auto_examples/03_connectivity/plot_signal_extraction.html#sphx-glr-auto-examples-03-connectivity-plot-signal-extraction-py)\n",
    "Part of this script (and future updates) takes inspiration from: https://github.com/brain-modelling-group/fmripop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 - Generating a virtual environment to run the analyses\n",
    "\n",
    "First, I generated a Python virtual environment to ensure reproducibility. This [page](https://anbasile.github.io/posts/2017-06-25-jupyter-venv/) briefly explains how to create a Python virtual environment, and set it as an iPython kernel. This allows you to reuse the exact method in your own Jupyter notebook. This GitHub includes a ```requirements.txt``` file that can be used when setting your environment. Here are the basic commands to reproduce in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m venv projectname\n",
    "#source projectname/bin/activate\n",
    "#pip install -r requirements.txt #Including the ipykernel package\n",
    "#ipython kernel install --user --name=projectname\n",
    "\n",
    "#On Jupyter, you should now be able to change the Kernel to the specific environnment specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Importing the modules necessary for analyses\n",
    "\n",
    "As a personnal preference, I like to install all packages first at the top in order of first appearance in the code and separate by packages, then, if needed, refer later in the code with a comment if there is confusion. Here are the packages we need and their usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nilearn packages:\n",
    "from nilearn import datasets #For atlases\n",
    "from nilearn import plotting #To plot brain images\n",
    "from nilearn.input_data import NiftiLabelsMasker #To mask the data\n",
    "from nilearn.connectome import ConnectivityMeasure #To compute the connectivity matrices\n",
    "\n",
    "#Various packages\n",
    "import os #To create directories \n",
    "import pandas as pd #For dataframe manipulation (e.g. confound file)\n",
    "import numpy as np #To conversion to numpy array of the matrix\n",
    "from matplotlib import pyplot as plt #Used to bypass a bug where the figures wouldn't close in Nilearn in my script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Importing the brain atlas\n",
    "\n",
    "For this tutorial, I chose to use the [Schaefer (2018)](https://github.com/ThomasYeoLab/CBIG/blob/master/stable_projects/brain_parcellation/Schaefer2018_LocalGlobal/Parcellations/Updates/Update_20190916_README.md) atlas. Note that not all atlases are configured in the same manner, so refer to the specific atlas you intend to use in Nilearn's help page [here](https://nilearn.github.io/modules/reference.html#module-nilearn.datasets)\n",
    "\n",
    "The code below imports the atlases. We first extract the 'maps' (i.e. the .nii image representing the atlas regions) and the we extract the labels (i.e. what each region is) from the downloaded dictionary.\n",
    "\n",
    "Note that the original 400 labels used by default in the atlas and the 300 labels did not work in the script. The length of the labels and the lenght of the matrix did not match which did not allow the code to work. I haven't had the time to explore the issue further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The atlas is located at /Users/stong3/nilearn_data/schaefer_2018/Schaefer2018_200Parcels_7Networks_order_FSLMNI152_1mm.nii.gz\n"
     ]
    }
   ],
   "source": [
    "atlas_schaefer = datasets.fetch_atlas_schaefer_2018(n_rois=200)\n",
    "atlas_filename_schaefer = atlas_schaefer.maps \n",
    "labels_schaefer = atlas_schaefer.labels \n",
    "print(f'The atlas is located at {atlas_filename_schaefer}')\n",
    "#print(labels_schaefer) #Prints the array of the labels, if needed\n",
    "#print(len(labels_schaefer)) #Prints the length of the labels, if needed\n",
    "#plotting.plot_roi(atlas_filename_schaefer) #Plotting the regions included in the atlas, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Extraction preparation\n",
    "\n",
    "Before we can procede to the extraction, we need to set a few things.\n",
    "\n",
    "1) We need to set the location of our subjects (i.e. where Python can find our images and the confound files)\n",
    "2) Set a location for Python to output our derivatives (i.e. connectivity matrices)\n",
    "3) Set the variables for extraction (subjects, session, confounds, kind of matrix, atlas)\n",
    "4) Start the loops for extraction\n",
    "\n",
    "To note, in future versions, the lists of variables might be replaced by argument parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/connectivity_matrices/ already exists. No directory is created.\n",
      "------------------------------------\n",
      "Description of the post-processing: \n",
      "    Subjects to process are with the following IDs : ['00001']\n",
      "    Sessiong to process are the following : ['BL00A', 'FU12A']\n",
      "    The confounds included to generate the matrices are : ['csf', 'white_matter', 'global_signal', 'trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z', 'cosine00', 'cosine01', 'cosine02']\n",
      "    The kind of correlation matrices to be generated are: ['correlation', 'partial correlation']\n",
      "    All procedures will be done with the schaefer atlas/map.\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1)\n",
    "subjects_location = '/Users/stong3/Desktop/test_fmriprep_PAD/derivatives/fmriprep/fmriprep/'\n",
    "\n",
    "# 2)\n",
    "connectivity_matrices_dir = '/Users/stong3/Desktop/test_fmriprep_PAD/derivatives/connectivity_matrices/'\n",
    "\n",
    "#This part creates the root directory inside of the derivatives folder, specified by the BIDS convention. The path above should reflect where you want the directory to go.\n",
    "#Even if the intermediate path (derivatives) is not created yet, the function 'makedirs' creates it for you.\n",
    "\n",
    "#The function below tests whether the path already exists. If not, it creates it for you.\n",
    "if not os.path.exists(connectivity_matrices_dir):\n",
    "    os.makedirs(connectivity_matrices_dir)\n",
    "    print(f'Created directory:{connectivity_matrices_dir}')\n",
    "else:\n",
    "    print(f'Directory {connectivity_matrices_dir} already exists. No directory is created.')\n",
    "    \n",
    "\n",
    "#3)\n",
    "subject_list = ['00001']\n",
    "session_list = ['BL00A', 'FU12A']\n",
    "list_confounds = ['csf', 'white_matter', 'global_signal', 'trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z', 'cosine00', 'cosine01', 'cosine02']\n",
    "kind_connectivity = ['correlation', 'partial correlation']\n",
    "atlas = 'schaefer'\n",
    "\n",
    "print('------------------------------------')\n",
    "print('Description of the post-processing: ')\n",
    "print(f'    Subjects to process are with the following IDs : {subject_list}')\n",
    "print(f'    Sessiong to process are the following : {session_list}')\n",
    "print(f'    The confounds included to generate the matrices are : {list_confounds}')\n",
    "print(f'    The kind of correlation matrices to be generated are: {kind_connectivity}')\n",
    "print(f'    All procedures will be done with the {atlas} atlas/map.')\n",
    "print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the loops! We will create 3 nested loops. \n",
    "\n",
    "The first will iterate over all subjects in the list we gave our script. \n",
    "\n",
    "The second will, for every subject, iterate over all the sessions that are available. In this section, we will set the location of our files. Then, we will use Pandas to reduce the raw confound regressor files to only the confounds we want. Finally, we will created our final time_series object, which will have our regressors modeled out.\n",
    "\n",
    "The final loop will create connectivity matrices for all the kinds that are specified, for every session, within every subjects. In this last loop, we will save the connectivity matrices as .csv files and as images. Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connectivity matrices extraction for subject 00001 for session BL00A\n",
      "------------------------------------\n",
      " \n",
      "Fetching the paths for the fMRI files and the confond files...\n",
      "The path for the fmri file is: /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/fmriprep/fmriprep/sub-00001/ses-BL00A/func/sub-00001_ses-BL00A_task-rest_run-1_space-T1w_desc-preproc_bold.nii.gz\n",
      "The path for the full confound file is: /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/fmriprep/fmriprep/sub-00001/ses-BL00A/func/sub-00001_ses-BL00A_task-rest_run-1_desc-confounds_regressors.tsv\n",
      "------------------------------------\n",
      " \n",
      "We now import the confound file using Pandas:\n",
      "Confounds selected for this extractions were: ['csf', 'white_matter', 'global_signal', 'trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z', 'cosine00', 'cosine01', 'cosine02']\n",
      "Conversion to Numpy array:\n",
      "Done!\n",
      "------------------------------------\n",
      "Creating masker using  /Users/stong3/nilearn_data/schaefer_2018/Schaefer2018_200Parcels_7Networks_order_FSLMNI152_1mm.nii.gz\n",
      "[NiftiLabelsMasker.fit_transform] loading data from /Users/stong3/nilearn_data/schaefer_2018/Schaefer2018_200Parcels_7Networks_order_FSLMNI152_1mm.nii.gz\n",
      "Resampling labels\n",
      "[NiftiLabelsMasker.transform_single_imgs] Loading data from /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/fmriprep/fmriprep/sub-00001/ses-BL00A/func/sub-00001_ses-BL00A_task-rest_run-1_space-T1w_desc-preproc_bold.nii.gz\n",
      "[NiftiLabelsMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiLabelsMasker.transform_single_imgs] Cleaning extracted signals\n",
      "The shape of our time series is:  (150, 200)\n",
      "Extracting connectivity matrices for the following kinds:  ['correlation', 'partial correlation']\n",
      "The shape of the correlation matrix is:  (200, 200)\n",
      "Testing if the shape of the matrix and the lenght of the labels are matching:\n",
      "The shape of the matrix and labels are matching.\n",
      "Creating a Pandas dataframe with labels as index\n",
      "Creating a directory to save the computed correlation matrices\n",
      "Directory /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/connectivity_matrices/sub-00001/ses-BL00A/kind-correlation/ already exists. None is created.\n",
      "Saving dataframe to a .csv file in : /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/connectivity_matrices/sub-00001/ses-BL00A/kind-correlation/\n",
      "The shape of the correlation matrix is:  (200, 200)\n",
      "Testing if the shape of the matrix and the lenght of the labels are matching:\n",
      "The shape of the matrix and labels are matching.\n",
      "Creating a Pandas dataframe with labels as index\n",
      "Creating a directory to save the computed correlation matrices\n",
      "Directory /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/connectivity_matrices/sub-00001/ses-BL00A/kind-partial correlation/ already exists. None is created.\n",
      "Saving dataframe to a .csv file in : /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/connectivity_matrices/sub-00001/ses-BL00A/kind-partial correlation/\n",
      "Starting connectivity matrices extraction for subject 00001 for session FU12A\n",
      "------------------------------------\n",
      " \n",
      "Fetching the paths for the fMRI files and the confond files...\n",
      "The path for the fmri file is: /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/fmriprep/fmriprep/sub-00001/ses-FU12A/func/sub-00001_ses-FU12A_task-rest_run-1_space-T1w_desc-preproc_bold.nii.gz\n",
      "The path for the full confound file is: /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/fmriprep/fmriprep/sub-00001/ses-FU12A/func/sub-00001_ses-FU12A_task-rest_run-1_desc-confounds_regressors.tsv\n",
      "------------------------------------\n",
      " \n",
      "We now import the confound file using Pandas:\n",
      "Confounds selected for this extractions were: ['csf', 'white_matter', 'global_signal', 'trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z', 'cosine00', 'cosine01', 'cosine02']\n",
      "Conversion to Numpy array:\n",
      "Done!\n",
      "------------------------------------\n",
      "Creating masker using  /Users/stong3/nilearn_data/schaefer_2018/Schaefer2018_200Parcels_7Networks_order_FSLMNI152_1mm.nii.gz\n",
      "[NiftiLabelsMasker.fit_transform] loading data from /Users/stong3/nilearn_data/schaefer_2018/Schaefer2018_200Parcels_7Networks_order_FSLMNI152_1mm.nii.gz\n",
      "Resampling labels\n",
      "[NiftiLabelsMasker.transform_single_imgs] Loading data from /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/fmriprep/fmriprep/sub-00001/ses-FU12A/func/sub-00001_ses-FU12A_task-rest_run-1_space-T1w_desc-preproc_bold.nii.gz\n",
      "[NiftiLabelsMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiLabelsMasker.transform_single_imgs] Cleaning extracted signals\n",
      "The shape of our time series is:  (150, 200)\n",
      "Extracting connectivity matrices for the following kinds:  ['correlation', 'partial correlation']\n",
      "The shape of the correlation matrix is:  (200, 200)\n",
      "Testing if the shape of the matrix and the lenght of the labels are matching:\n",
      "The shape of the matrix and labels are matching.\n",
      "Creating a Pandas dataframe with labels as index\n",
      "Creating a directory to save the computed correlation matrices\n",
      "Directory /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/connectivity_matrices/sub-00001/ses-FU12A/kind-correlation/ already exists. None is created.\n",
      "Saving dataframe to a .csv file in : /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/connectivity_matrices/sub-00001/ses-FU12A/kind-correlation/\n",
      "The shape of the correlation matrix is:  (200, 200)\n",
      "Testing if the shape of the matrix and the lenght of the labels are matching:\n",
      "The shape of the matrix and labels are matching.\n",
      "Creating a Pandas dataframe with labels as index\n",
      "Creating a directory to save the computed correlation matrices\n",
      "Directory /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/connectivity_matrices/sub-00001/ses-FU12A/kind-partial correlation/ already exists. None is created.\n",
      "Saving dataframe to a .csv file in : /Users/stong3/Desktop/test_fmriprep_PAD/derivatives/connectivity_matrices/sub-00001/ses-FU12A/kind-partial correlation/\n"
     ]
    }
   ],
   "source": [
    "for subject in subject_list:\n",
    "    for session in session_list:\n",
    "        print(f'Starting connectivity matrices extraction for subject {subject} for session {session}')\n",
    "        print('------------------------------------')\n",
    "        print(' ')\n",
    "        \n",
    "        #First, we need to import the paths where the fMRI pre-processed and confounds files are stored. They follow BIDS convention and depend on the path provided\n",
    "        #for the subject location. After fetching the paths, we print them.\n",
    "        print('Fetching the paths for the fMRI files and the confond files...')\n",
    "        pre_processed_fmri_file = f'{subjects_location}sub-{subject}/ses-{session}/func/sub-{subject}_ses-{session}_task-rest_run-1_space-T1w_desc-preproc_bold.nii.gz'\n",
    "        full_confound_file_fmriprep = f'{subjects_location}sub-{subject}/ses-{session}/func/sub-{subject}_ses-{session}_task-rest_run-1_desc-confounds_regressors.tsv'\n",
    "\n",
    "        print(f'The path for the fmri file is: {pre_processed_fmri_file}')\n",
    "        print(f'The path for the full confound file is: {full_confound_file_fmriprep}')\n",
    "        print('------------------------------------')\n",
    "        print(' ')\n",
    "\n",
    "        #The confound file from fMRIPrep is huge and needs to be cleaned. We import it with Pandas to keep column names and select them more easily.\n",
    "        print('We now import the confound file using Pandas:')\n",
    "        confounds = pd.read_csv(full_confound_file_fmriprep, delimiter = '\\t')\n",
    "        print(f'Confounds selected for this extractions were: {list_confounds}')\n",
    "        #print(confounds.head())\n",
    "        final_confounds = confounds[list_confounds]\n",
    "        #print(final_confounds.head())\n",
    "\n",
    "        #We need to convert the dataframe type of Pandas to a Numpy array so it is readable by Nilearn.\n",
    "        print('Conversion to Numpy array:')\n",
    "        confounds_np = final_confounds.to_numpy()\n",
    "        print('Done!')\n",
    "        print('------------------------------------')\n",
    "        \n",
    "\n",
    "        #We are now ready to extract the time series using our atlas mask\n",
    "        #from nilearn.input_data import NiftiLabelsMasker #Already imported in the top of the script\n",
    "        print('Creating masker using ', atlas_filename_schaefer)\n",
    "\n",
    "        masker = NiftiLabelsMasker(labels_img=atlas_filename_schaefer, standardize=True, verbose=5)\n",
    "        time_series = masker.fit_transform(pre_processed_fmri_file, confounds=confounds_np)\n",
    "        print('The shape of our time series is: ', np.shape(time_series))\n",
    "\n",
    "        #We are now ready to extract the connectivity matrix using Nilearn functions.\n",
    "        #This part of the script will:\n",
    "        ## 1) Create a loop to extract matrices according to the kind of matrix wanted (e.g. correlation, partial correlation, etc.)\n",
    "        ## 2) Check that the matrix shape matches the length of the atlas labels\n",
    "        ## 3) Create a Pandas dataframe with the labels as columns and indices\n",
    "        ## 4) Create a directory where we can save the matrix ('bids-like')\n",
    "        ## 5) Export the dataframe to a .csv file \n",
    "        ## 6) Use Nilearn functions to plot the matrix with labels\n",
    "        ## 7) Save the image in the same directory as the .csv file.\n",
    "\n",
    "        ### 1)\n",
    "        print('Extracting connectivity matrices for the following kinds: ', kind_connectivity)\n",
    "        for kind in kind_connectivity:\n",
    "            correlation_measure = ConnectivityMeasure(kind = kind)\n",
    "            correlation_matrix = correlation_measure.fit_transform([time_series])[0]\n",
    "            print('The shape of the correlation matrix is: ', np.shape(correlation_matrix))\n",
    "        ### 2)\n",
    "            print('Testing if the shape of the matrix and the lenght of the labels are matching:')\n",
    "            try:\n",
    "                len(labels_schaefer) in np.shape(correlation_matrix)\n",
    "                if False:\n",
    "                    raise ValueError('The length of the labels do not match the shape of the correlation_matrix')\n",
    "            except ValueError:\n",
    "                exit('The shape of the matrix and labels are not matching')\n",
    "            print('The shape of the matrix and labels are matching.')\n",
    "        ### 3)\n",
    "            print('Creating a Pandas dataframe with labels as index')\n",
    "            subject_connectivity_matrix = pd.DataFrame(data=correlation_matrix, index=labels_schaefer, columns=labels_schaefer)\n",
    "            #print(subject_connectivity_matrix.head())\n",
    "        ### 4)\n",
    "            print('Creating a directory to save the computed correlation matrices')\n",
    "            dir_matrices_derivatives = f'{connectivity_matrices_dir}sub-{subject}/ses-{session}/kind-{kind}/'\n",
    "            if not os.path.exists(dir_matrices_derivatives):\n",
    "                os.makedirs(dir_matrices_derivatives)\n",
    "                print(f'Created directory:{dir_matrices_derivatives}')\n",
    "            else:\n",
    "                print(f'Directory {dir_matrices_derivatives} already exists. None is created.')\n",
    "\n",
    "        ### 5)\n",
    "            #Using an f string, we give the new directory\n",
    "            print(f'Saving dataframe to a .csv file in : {dir_matrices_derivatives}')\n",
    "            subject_connectivity_matrix.to_csv(f'{dir_matrices_derivatives}sub-{subject}_ses-{session}_atlas-{atlas}_kind-{kind}_connectivity_matrix.csv')\n",
    "\n",
    "        ### 6)\n",
    "            #from nilearn import plotting\n",
    "            ## This one plots the matrix without reordering the clusters of fMRI activation and fitting labels\n",
    "            display = plotting.plot_matrix(correlation_matrix)\n",
    "            display.figure.savefig(f'{dir_matrices_derivatives}sub-{subject}_ses-{session}_atlas-{atlas}_kind-{kind}_connectivity_matrix_no_labels_not_ordered.png')  \n",
    "            plt.close()\n",
    "\n",
    "            ## This one plots the matrix, reorders the clusters and places the labels on the figure\n",
    "            display1 = plotting.plot_matrix(correlation_matrix, figure=(30, 30), labels=labels_schaefer, vmax=0.8, vmin=-0.8, reorder=True)\n",
    "            display1.figure.savefig(f'{dir_matrices_derivatives}sub-{subject}_ses-{session}_atlas-{atlas}_kind-{kind}_connectivity_matrix_labels_ordered.png')  \n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code therefore creates 3 outputs, in 'bids-like' format, per session, per kind of matrix, for each subject. The resulting files should look something like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "connectivity_matrices/\n",
    "└── sub-00001\n",
    "    ├── ses-BL00A\n",
    "    │   ├── kind-correlation\n",
    "    │   │   ├── sub-00001_ses-BL00A_atlas-schaefer_kind-correlation_connectivity_matrix.csv\n",
    "    │   │   ├── sub-00001_ses-BL00A_atlas-schaefer_kind-correlation_connectivity_matrix_labels_ordered.png\n",
    "    │   │   └── sub-00001_ses-BL00A_atlas-schaefer_kind-correlation_connectivity_matrix_no_labels_not_ordered.png\n",
    "    │   └── kind-partial\\ correlation\n",
    "    │       ├── sub-00001_ses-BL00A_atlas-schaefer_kind-partial\\ correlation_connectivity_matrix.csv\n",
    "    │       ├── sub-00001_ses-BL00A_atlas-schaefer_kind-partial\\ correlation_connectivity_matrix_labels_ordered.png\n",
    "    │       └── sub-00001_ses-BL00A_atlas-schaefer_kind-partial\\ correlation_connectivity_matrix_no_labels_not_ordered.png\n",
    "    └── ses-FU12A\n",
    "        ├── kind-correlation\n",
    "        │   ├── sub-00001_ses-FU12A_atlas-schaefer_kind-correlation_connectivity_matrix.csv\n",
    "        │   ├── sub-00001_ses-FU12A_atlas-schaefer_kind-correlation_connectivity_matrix_labels_ordered.png\n",
    "        │   └── sub-00001_ses-FU12A_atlas-schaefer_kind-correlation_connectivity_matrix_no_labels_not_ordered.png\n",
    "        └── kind-partial\\ correlation\n",
    "            ├── sub-00001_ses-FU12A_atlas-schaefer_kind-partial\\ correlation_connectivity_matrix.csv\n",
    "            ├── sub-00001_ses-FU12A_atlas-schaefer_kind-partial\\ correlation_connectivity_matrix_labels_ordered.png\n",
    "            └── sub-00001_ses-FU12A_atlas-schaefer_kind-partial\\ correlation_connectivity_matrix_no_labels_not_ordered.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, this project aimed to start from un-processed data in BIDS format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bhs_project",
   "language": "python",
   "name": "bhs_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
